"""
This type stub file was generated by pyright.
"""

from typing import Any

class SentencePieceProcessor:
    def __init__(self, *args: Any) -> None: ...
    def get_piece_size(self) -> int:
        """
        Vocab size
        """
        ...

    def id_to_piece(self, *args: Any) -> str:
        """
        >>> sp.id_to_piece(209)
        il
        """
        ...

    def piece_to_id(self, *args: Any) -> int:
        """
        >>> sp.piece_to_id('▁This')
        208
        """
        ...

    def is_control(self, *args: Any) -> int:
        """
        <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)
        <s> and </s> are defined as 'control' symbol.
        >>> for id in range(3):
        ...   print(sp.id_to_piece(id), sp.is_control(id))
        <unk> False
        <s> True
        </s> True
        """
        ...

    def encode_as_pieces(self, *args: Any) -> list[str]:
        """
        >>> sp.encode_as_pieces('This is a test')
        ['▁This', '▁is', '▁a', '▁t', 'est']
        """
        ...

    def encode_as_ids(self, *args: Any) -> list[int]:
        """
        >>> sp.encode_as_ids('This is a test')
        [208, 31, 9, 434, 601]
        """
        ...

    def decode_pieces(self, *args: Any) -> str:
        """
        >>> sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']
        This is a test
        """
        ...

    def decode_ids(self, *args: Any) -> str:
        """
        >>> sp.decode_ids([209, 31, 9, 375, 586])
        il is a con live
        """
        ...

    def load(self, *args: Any) -> None:
        """
        >>> sp.load('m.model')
        """
        ...

    def load_from_serialized_proto(self, *args: Any) -> None:
        """
        serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()

        sp = spm.SentencePieceProcessor()
        sp.load_from_serialized_proto(serialized_model_proto)
        """
        ...

    def bos_id(self) -> int: ...
    def eos_id(self) -> int: ...
    def unk_id(self) -> int:
        """
        UNK - "unknown token" - is used to replace the rare words that did not fit in your vocabulary. So your sentence My name is guotong1988 will be translated into My name is _unk_.
        """
        ...

    def pad_id(self) -> int:
        """
        PAD - your GPU (or CPU at worst) processes your training data in batches and all the sequences in your batch should have the same length. If the max length of your sequence is 8, your sentence My name is guotong1988 will be padded from either side to fit this length: My name is guotong1988 _pad_ _pad_ _pad_ _pad_
        >>> print('pad=', sp.pad_id())  # disabled by default
        pad= -1
        """
        ...

    def sample_encode_as_pieces(self, *args: Any) -> list[str]: ...
    def sample_encode_as_ids(self, *args: Any) -> list[int]: ...
    def nbest_encode_as_pieces(self, *args: Any) -> list[list[str]]: ...
    def nbest_encode_as_ids(self, *args: Any) -> list[list[int]]: ...
    def set_vocabulary(self, *args: Any) -> None:
        """
        vocabs = list(filter(lambda x: x in freq and freq[x] > 1000, vocabs))
        sp.set_vocabulary(vocabs)
        """
        ...

    def reset_vocabulary(self) -> None: ...

class SentencePieceTrainer:
    @staticmethod
    def train(*args: Any) -> Any:
        """
        >>> spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')
        >>> spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')
        >>> spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000')
        >>> spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m')
        >>> spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m --unk_surface=__UNKNOWN__') # unk surface defaults to U+2047
        >>> spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]')
        # Disable BOS/EOS
        >>> spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m --bos_id=-1 --eos_id=-1')
        # When **--model_type=unigram** (default) is used,  we can perform sampling and n-best segmentation for data augmentation. See subword regularization paper [[kudo18]](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8) for more detail.
        spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000')
        # Sentencepiece supports BPE (byte-pair-encoding) for subword segmentation with **--model_type=bpe** flag.   We do not find empirical differences in translation quality between BPE and unigram model, but unigram model can perform sampling and n-best segmentation. See subword regularization paper [[kudo18]](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8) for more detail.
        spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')
        # NFKC normalization and lower casing.
        >>> spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_name=nfkc_cf')
        >>> spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --input_sentence_size=1000')
        >>> spm.SentencePieceTrainer.train('--input=word_freq_list.tsv --input_format=tsv --model_prefix=m --vocab_size=2000')
        """
        ...
